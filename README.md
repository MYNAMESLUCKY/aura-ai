# ğŸ¤– Aura â€” Local AI Assistant with Persistent Memory

Aura is a privacy-first, local AI assistant powered by Ollama.
It features **persistent identity**, **long-term memory**, **semantic (RAG) recall**, 
**automated browser control**, and **optional live web search** â€” all stored locally on your machine.

---

## âœ¨ Key Features

- ğŸ” **Persistent Identity**
  - One-time generated user ID
  - Memory survives restarts
  - Encrypted identity storage

- ğŸ§  **Hybrid Memory System**
  - Short-term chat memory
  - Long-term factual memory
  - Semantic vector memory (RAG)
  - Automatic memory summarization

- ğŸŒ **Smart Browser Automation**
  - Intelligent intent parsing with LLM
  - Automatic web search integration
  - Reliable URL extraction and opening
  - Multi-platform support (YouTube, Netflix, GitHub, etc.)
  - Comprehensive error handling

- ğŸ” **Optional Web Search**
  - Uses Tavily Search API
  - Automatic fallback mechanisms
  - Disabled by default unless API key is set

- âš™ï¸ **Model Agnostic**
  - Uses Ollama for local inference
  - Switch models anytime
  - Works with any Ollama-compatible model

- ğŸ©º **Built-in Diagnostics**
  - Doctor command for system health checks
  - Config migration and validation
  - Model inspection and verification

---

## ğŸ“¦ Installation

### 1. Prerequisites
- **Python 3.10+**
- **Ollama installed and running** (https://ollama.ai)
- A supported model pulled: `ollama pull deepseek-v3.1:671b-cloud`

### 2. Clone and Setup

```bash
# Clone the repository
git clone <your-repo>
cd cyber/aura

# Create virtual environment
python -m venv .venv

# Activate virtual environment
# Windows:
.venv\Scripts\activate

# macOS/Linux:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
pip install -e .
```

### 3. Configuration

Create `aura/config.yaml` with your settings:

```yaml
# LLM Configuration
model: "deepseek-v3.1:671b-cloud"  # Ollama model name
temperature: 0.7

# Memory Settings
memory_dir: "./memory"
vector_db_dir: "./db/vectors"

# Web Search (Optional)
tavily_api_key: ""  # Set your Tavily API key here
enable_web_search: false  # Enable with API key

# Identity
identity_dir: "./identity"
```

---

## ğŸš€ Running Aura

### Start the Agent

```bash
aura
```

### Check Ollama is Running

Before running Aura, ensure Ollama is running:

```bash
# Check if Ollama is accessible
curl http://localhost:11434/api/tags

# Or pull a model if needed
ollama pull deepseek-v3.1:671b-cloud
```

### Basic Usage

```
You: "What is the capital of France?"
Aura: "The capital of France is Paris..."

You: "Open YouTube and play lo-fi music"
Aura: "ğŸŒ Opened result: https://youtube.com/..."

You: "Search for GitHub repositories about AI"
Aura: "Based on web search..."
```

---

## ğŸŒ Browser Automation Features

### How It Works

Aura intelligently detects and executes browser-related requests:

1. **Intent Parsing** - LLM extracts structured browser intent as JSON
2. **Direct Execution** - Opens URL if directly provided
3. **Web Search Fallback** - Uses Tavily to find relevant URLs
4. **URL Extraction** - Regex extracts valid URLs from search results
5. **Error Handling** - Detailed logging for debugging

### Intent Schema

The browser intent follows this JSON schema:

```json
{
  "tool": "browser",
  "action": "open|search|play",
  "query": "<what should be opened or searched>"
}
```

### Supported Actions

- **`open`** - Open a website or search result
- **`search`** - Perform a web search
- **`play`** - Play media (YouTube, Spotify, etc.)

### Supported Platforms

- YouTube
- Netflix
- Spotify
- GitHub
- Wikipedia
- Google
- And any other website

### Example Usage in Code

```python
from aura.tools.browser.intent import parse_browser_intent
from aura.tools.browser.execute import execute_browser_intent
from langchain_ollama import ChatOllama

llm = ChatOllama(model='deepseek-v3.1:671b-cloud', temperature=0)

# Parse user input into browser intent
intent = parse_browser_intent(llm, 'open youtube and play lo-fi music')
print('INTENT:', intent)
# Output: {'tool': 'browser', 'action': 'open', 'query': 'lo-fi music'}

# Execute the intent
if intent:
    result = execute_browser_intent(intent)
    print(result)
    # Output: ğŸŒ Opened result: https://youtube.com/...
```

### Error Handling

The browser tool includes comprehensive error handling:

- âœ… Validates URLs before opening
- âœ… Catches web search failures
- âœ… Logs detailed error messages
- âœ… Provides fallback mechanisms
- âœ… Never fails silently

**Example Error Messages:**
```
âš ï¸ Browser intent parsing failed: <error details>
âš ï¸ Web search unavailable: <error details>
âš ï¸ Failed to open browser: <error details>
âš ï¸ Couldn't find a suitable link to open.
```

---

## ğŸ“ Browser Tool Architecture

### File Structure

| File | Purpose |
|------|---------|
| `browser/intent.py` | Parse user input into structured browser intent |
| `browser/execute.py` | Execute browser intents with fallbacks |
| `browser/enrich.py` | Resolve intent to safe URLs using Tavily + LLM |
| `browser/open.py` | Launch browsers with URL normalization |
| `browser/detect.py` | Detect installed browsers on Windows |
| `intent.py` | Pre-filtering for browser vs non-browser requests |
| `registry.py` | Tool routing and orchestration |

### Execution Flow Diagram

```
User Input
    â†“
parse_browser_intent() [browser/intent.py]
    â†“ (LLM extracts structured intent)
execute_browser_intent() [browser/execute.py]
    â†“
[Has direct URL?] â†’ Yes â†’ webbrowser.open(url)
    â†“ No
run_web_search() [features/web_search.py]
    â†“ (Tavily search)
_URL_REGEX.findall() 
    â†“ (Extract URLs)
webbrowser.open(urls[0])
    â†“
Success Response / Error Logging
```

---

## ğŸ—ï¸ Project Structure

```
aura/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ aura/
â”‚       â”œâ”€â”€ __main__.py           # Entry point
â”‚       â”œâ”€â”€ app.py                # Main application
â”‚       â”œâ”€â”€ config.py             # Configuration management
â”‚       â”œâ”€â”€ db.py                 # Database layer
â”‚       â”œâ”€â”€ identity.py           # User identity management
â”‚       â”œâ”€â”€ embeddings.py         # Vector embeddings
â”‚       â”œâ”€â”€ migrator.py           # Data migration
â”‚       â”œâ”€â”€ ollama_utils.py       # Ollama integration
â”‚       â”œâ”€â”€ version.py            # Version info
â”‚       â”œâ”€â”€ features/
â”‚       â”‚   â”œâ”€â”€ chat.py           # Chat interface
â”‚       â”‚   â”œâ”€â”€ memory.py         # Memory management
â”‚       â”‚   â”œâ”€â”€ vector_memory.py  # RAG vector storage
â”‚       â”‚   â”œâ”€â”€ user_memory.py    # User-specific memory
â”‚       â”‚   â”œâ”€â”€ summarizer.py     # Memory summarization
â”‚       â”‚   â”œâ”€â”€ web_search.py     # Tavily web search
â”‚       â”‚   â””â”€â”€ tools.py          # Tool orchestration
â”‚       â””â”€â”€ tools/
â”‚           â”œâ”€â”€ intent.py         # Intent detection
â”‚           â”œâ”€â”€ registry.py       # Tool routing
â”‚           â””â”€â”€ browser/
â”‚               â”œâ”€â”€ intent.py     # Browser intent parsing
â”‚               â”œâ”€â”€ execute.py    # Browser execution
â”‚               â”œâ”€â”€ enrich.py     # Intent enrichment
â”‚               â”œâ”€â”€ open.py       # Browser launcher
â”‚               â””â”€â”€ detect.py     # Browser detection
â”œâ”€â”€ config.yaml                   # Configuration file
â””â”€â”€ identity/
    â”œâ”€â”€ user_id.txt              # User identifier
    â””â”€â”€ private_key.enc          # Encrypted identity key
```

---

## ğŸ”§ Development

### Running Tests

```bash
pytest tests/
```

### Debugging

Enable debug logging:

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

### Adding New Tools

1. Create a new tool module in `src/aura/tools/`
2. Implement intent parsing and execution
3. Register in `tools/registry.py`

Example structure:

```python
# src/aura/tools/calculator.py
def parse_calculator_intent(llm, user_input: str) -> dict | None:
    """Extract mathematical intent from user input"""
    pass

def execute_calculator_intent(intent: dict) -> str:
    """Execute the calculation"""
    pass
```

---

## ğŸ› Troubleshooting

### Browser Not Opening

**Issue**: Browser sometimes opens and sometimes doesn't

**Root Causes** (Fixed in v0.1.1):
- ~~Incorrect JSON schema validation~~ âœ… Fixed
- ~~Missing error handling~~ âœ… Fixed
- ~~Silent failures~~ âœ… Fixed with logging

**Current Solutions**:
1. Check Tavily API key is set (if using web search)
2. Ensure system default browser is configured
3. Check console output for error messages
4. Verify network connectivity for web search
5. Ensure Ollama is running on localhost:11434

### Memory Not Persisting

**Issue**: Memory is lost after restart

**Solutions**:
1. Verify `memory_dir` exists in config
2. Check file permissions on memory directory
3. Ensure database is properly initialized

### Ollama Connection Issues

**Issue**: "Failed to connect to Ollama"

**Solutions**:
1. Verify Ollama is running: `ollama list`
2. Check Ollama is on correct host/port in config (default: localhost:11434)
3. Ensure model is pulled: `ollama pull <model-name>`
4. Restart Ollama service

---

## ğŸ“‹ Recent Updates (v0.1.1)

### Browser Tool Improvements âœ¨

- âœ… **Fixed Intent Validation** - Corrected JSON schema from `platform` to `action`
- âœ… **Comprehensive Error Handling** - All exceptions caught and logged
- âœ… **Web Search Integration** - Better error handling for Tavily failures
- âœ… **URL Extraction** - Improved regex-based URL extraction from search results
- âœ… **Logging & Debugging** - Detailed error messages for troubleshooting

### What Was Fixed

1. **Intent Parsing** - Was checking for non-existent `platform` field, now validates `action` field
2. **Execution** - Added try/except blocks with proper error messages
3. **Web Search** - No more silent failures, all errors are logged
4. **Platform Filtering** - Simplified to open first valid URL instead of platform-specific logic

### Platform Support

- âœ… YouTube playback
- âœ… Netflix navigation
- âœ… Spotify music
- âœ… GitHub repository browsing
- âœ… Wikipedia searches
- âœ… Generic website opening

---

## ğŸ“ License

MIT License - See LICENSE file for details

---

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Submit a pull request with detailed description

---

## ğŸ“ Support

For issues or questions:
1. Check the troubleshooting section above
2. Review error logs/console output for detailed messages
3. Verify Ollama is running before reporting issues
4. Open an issue with reproduction steps
